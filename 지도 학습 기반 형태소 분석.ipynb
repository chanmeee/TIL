{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 지도 학습 기반 형태소 분석\n",
    "- 품질 좋은 임베딩을 만들기 위해서는 문장이나 단어의 경계를 컴퓨터에 알려줘야 한다. **그렇지 않으면 어휘 집합에 속한 단어 수가 기하급수적으로 늘어나서 연산의 비효율이 발생**한다.\n",
    "- 예를 들어 동사 '가다'의 활용형인 '가겠다', '가더라'도 모두 어휘 집합에 넣는다면 어휘 집합에 속한 단어 수는 총 2개가 된다. 이 방식이 문제는 새로운 활용형이 나타날 때마다 어휘 집합을 계속 늘려야한다는 점에 있다. '가겠더라', '갔다'라는 활용형도 말뭉치에 존재한다면 어휘 집합을 4개로 해야 한다. 이를 해결하기 위해 **형태소 분석 기법**을 사용한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KoNLPy 사용법\n",
    "- 은전한닢, 꼬꼬마, 한나눔, Okt, 코모란 등 5개 오픈소스 형태소 분석기를 파이썬 환경에서 사용할 수 있도록 인터페이스를 통일한 한국어 자연어 처리 패키지 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 은전한닢 분석기 \n",
    "from konlpy.tag import Mecab\n",
    "tokenizer = Mecab()\n",
    "tokenizer.morphs(\"아버지가방에들어가신다\")\n",
    "\n",
    "# 유의점: KoNLPy의 Mecab() 클래스는 Windows 환경에서 작동하지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 은전한닢 품사 정보 확인\n",
    "tokenizer.pos('아버지가방에들어가신다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['아버지', '가방', '에', '들어가', '시', 'ㄴ다']\n",
      "[('아버지', 'NNG'), ('가방', 'NNP'), ('에', 'JKB'), ('들어가', 'VV'), ('시', 'EP'), ('ㄴ다', 'EC')]\n"
     ]
    }
   ],
   "source": [
    "# get_tokenizer()\n",
    "from konlpy.tag import Okt, Komoran, Mecab, Hannanum, Kkma\n",
    "\n",
    "def get_tokenizer(tokenizer_name):\n",
    "    if tokenizer_name == 'okt':\n",
    "        tokenizer = Okt()\n",
    "    if tokenizer_name == 'komoran':\n",
    "        tokenizer = Komoran()\n",
    "    if tokenizer_name == 'mecab':\n",
    "        tokenizer = Mecab()\n",
    "    if tokenizer_name == 'hannanum':\n",
    "        tokenizer = Hannanum()\n",
    "    if tokenizer_name == 'kkma':\n",
    "        tokenizer = Kkma()\n",
    "    else:\n",
    "        tokenizer = Mecab()  # default \n",
    "    return tokenizer \n",
    "\n",
    "# 코모란 사용 예시\n",
    "tokenizer = get_tokenizer('komoran')\n",
    "print(tokenizer.morphs('아버지가방에들어가신다'))\n",
    "print(tokenizer.pos('아버지가방에들어가신다'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KoNLPy 내 분석기별 성능 차이 \n",
    "- 여러 가지 형태소 분석기의 성능을 비교해보면 은전한닢이 다른 분석기보다 속도가 빠른 것을 확인할 수 있다. \n",
    "- 속도만큼 중요한 것이 형태소 분석 품질이다. \n",
    "\n",
    "**어떤 형태소 분석기를 사용할지는 자신이 가진 데이터로 시험 삼아 형태소 분석을 해보고 속도나 품질을 비교해서 고르는 것이 좋다.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Khaiii 사용법\n",
    "- 카카오가 2018년 말 공개한 오픈소스 한국어 형태소 분석기\n",
    "- 국립국어우너이 구축한 세종 코퍼스를 이용해 CNN 모델을 적용해 학습했다. \n",
    "- 아키텍처 개요: 입력문장을 문자 단위로 읽어 들임 -> 컨볼루션 필터가 문자들을 슬라이딩해 가면서 정보 추출 -> 네트워크의 말단 레이어에서 이렇게 모은 정보들을 종합해 형태소의 경계와 품사 태그를 예측 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from khaiii import khaiiiApi\n",
    "tokenizer = khaiiiApi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tokenizer.analyze('아버지가방에들어가신다')\n",
    "tokens = []\n",
    "for word in data:\n",
    "    tokens.extend([str(m).split('/')[0] for m in word.morphs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 품사 정보 확인 \n",
    "data = tokenizer.analyze('아버지가방에들어가신다')\n",
    "tokens = []\n",
    "for word in data:\n",
    "    tokens.extend([str(m) for m in word.morphs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 은전한닢에 사용자 사전 추가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "tokenizer = Mecab()\n",
    "tokenizer.morphs('가우스전자 텔레비전 정말 좋네요')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "관심 단어인 ``가우스전자``가 의도치 않게 2개의 토큰으로 분석된 것을 확인할 수 있다. ``가우스전자`` 하나로 분석됐을 때보다 데이터 분석이나 임베딩 품질이 떨어질 가능성이 적지 않다. 따라서 **관심 단어들을 사용자 사전에 추가**해 ``가우스전자``같은 단어가 하나의 토큰으로 분석될 수 있도록 강제해야 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 우선 /notebooks/embedding/preprocessmecab-user-dic.csv 파일에 붙여 썼으면 하는 단어를 추가하기 \n",
    "\n",
    "# 은전한닢 사용자 사전 컴파일(bash)\n",
    "# bash preprocess.sh mecab-user-dic "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
