{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4장 단어 수준 임베딩\n",
    "## 4.1 NPLM\n",
    "\n",
    "Neural Probabilistic Language Model(NPLM)은 자연어 처리 분야에 임베딩 개념을 널리 퍼뜨리는 데 일조한 선구자적 모델로, 딥러닝의 아버지 요슈아 벤지오 연구 팀이 제안한 기법(Bengio et al., 2003)이다.\n",
    "\n",
    "\n",
    "### 모델 기본 구조\n",
    "NPLM은 '단어가 어떤 순서로 쓰였는가'에서 설명한 통계 기반의 전통적인 언어 모델의 한계를 극복하는 과정에서 탄생했다. Bengio et al.(2003)은 기존 언어 모델의 단점을 다음과 같이 설명했다.\n",
    "\n",
    "- 학습 데이터에 존재하지 않는 n-gram이 포함된 문장이 나타날 확률 값을 0으로 부여한다. 물론 **백오프**나 **스무딩**으로 이런 문제를 일부 보완할 수 있지만 완전한 것은 아니다.\n",
    "- 문장의 **장기 의존성(long-term dependency)**를 포착해내기 어렵다. 다시 말해 n-gram 모델의 n을 5 이상으로 길게 설정할 수 없다. n이 커질수록 그 등장 확률이 0인 단어 시퀀스가 기하급수적으로 늘어난다. \n",
    "- 단어/문장 간 유사도를 계산할 수 없다. \n",
    "\n",
    "### NPLM의 학습\n",
    "학습 파라미터의 종류가 많고 그 크기도 큰 편이다. \n",
    "\n",
    "## 4.2 Word2Vec\n",
    "2013년 구글 연구 팀이 발표한 기법으로 가장 널리 쓰이고 있는 단어 임베딩 모델이다. 두 개의 논문으로 나누어 발표했는데, 하나는 Skip-Gram과 CBOW라는 모델이 제안됐고 다른 하나는 이 두 모델을 근간으로 하되 네거티브 샘플링 등 학습 최적화 기법을 제안한 내용이 핵심 골자이다. \n",
    "\n",
    "### 모델 기본 구조\n",
    "- CBOW는 주변에 있는 **문맥 단어**들을 가지고 **타깃 단어** 하나를 맞추는 과정에서 학습된다. \n",
    "- Skip-gram 모델은 타깃 단어를 가지고 주변 문맥 단어가 무엇일지 예측하는 과정에서 학습된다. \n",
    "- CBOW의 경우 입력, 출력 학습 데이터 쌍이 {문맥 단어 4개, 타깃 단어} 하나인 반면, Skip-gram의 학습 데이터는 {타깃 단어, 타깃 직전 두 번째 단어}, {타깃 단어, 타깃 직전 단어}, {타깃 단어, 타깃 다음 단어}, {타깃 단어, 타깃 다음 두 번째 단어} 이렇게 4개쌍이 된다. Skip-gram이 같은 말뭉치로도 더 많은 학습 데이터를 확보할 수 있어서 임베딩 품질이 CBOW보다 좋은 경향이 있다. 따라서 Skip-gram 모델을 중심으로 Word2Vec 기법을 설명한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 튜토리얼\n",
    "\n",
    "#git pull origin master\n",
    "#bash preprocess.sh dump-tokenized\n",
    "\n",
    "#cd /notebooks/embedding\n",
    "#cat data/tokenized/wiki_ko_mecab.txt data/tokenized/ratings_mecab.txt data/tokenized/korquad_mecab.txt > data/tokenized/corpus_mecab.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec Skip-gram 모델 학습\n",
    "\n",
    "corpus_fname = '/notebooks/embedding/data/tokenized/corpus_mecab.txt'\n",
    "model_fname = '/notebooks/embedding/data/word-embbedings/word2vec/word2vec'\n",
    "\n",
    "from genism.models import Word2Vec\n",
    "\n",
    "corpus = [sent.strip().split(\" \") for sent in open(corpus_fname, 'r').readlines()]\n",
    "model = Word2Vec(corpus, size=100, workers=4, sg=1)\n",
    "model.save(model_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec Skip-gram 모델 학습 스크립트\n",
    "#cd /notebooks/embedding\n",
    "#python models/word_utils.py --method train_word2vec \\\n",
    "--input_path data/tokenized/corpus_mecab.txt \\\n",
    "--input_path data/word-embeddings/word2vec/word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코사인 유사도 상위 단어 목록 체크 코드\n",
    "\n",
    "from models.word_eval import WordEmbeddingEvaluator\n",
    "\n",
    "model = WordEmbeddingEvaluator(\"/notebooks/embedding/data/word-embbedings/word2vec/word2vec\", method=\"word2vec\", dim=100, tokenizer_name=\"mecab\")\n",
    "model.most_similar(\"희망\", topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 코사인 유사도 유의점\n",
    "코사인 유사도 값이 높다도 해서 두 단어가 유의 관계인 것은 아니다. 예를 들어 '덥다'와 '춥다'는 반의 관계이지만, '기온'이라는 속성을 매개로 강한 관련을 맺고 있기 때문에 코사인 유사도 값이 높다. **관련성**이 높은 단어를 출력한다는 의미로 이해하는 것이 좋다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
